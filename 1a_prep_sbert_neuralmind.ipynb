{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import joblib as jb\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "import json\n",
    "import dask\n",
    "\n",
    "import itertools\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(\"./data/sample_train.jl\", lines=True)\n",
    "test = pd.read_json(\"./data/sample_test.jl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_data = pd.read_json(\"./data/sample_item.jl\", lines=True)\n",
    "\n",
    "item_title_map = item_data[['title', 'item_id']].set_index(\"item_id\").squeeze().to_dict()\n",
    "item_data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, item_data):\n",
    "        self.item_data = item_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.item_data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = self.item_data.iloc[index]['title']\n",
    "\n",
    "        return title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instancia o modelo bert e prepara os dados dos items para uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "pretrained = 'neuralmind/bert-large-portuguese-cased'\n",
    "\n",
    "model = SentenceTransformer(pretrained,  device='cuda')\n",
    "train_data = Dataset(item_data)\n",
    "train_loader = DataLoader(train_data, batch_size=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria word embeding dos items e extrai as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embs_list = list()\n",
    "for data in tqdm.tqdm(train_loader):\n",
    "    embs = model.encode(data)\n",
    "    embs_list.append(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_np = np.vstack(embs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(embs_np, \"22a_embs_np.pkl.z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize a new index, using a HNSW index on Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "index.addDataPointBatch(data=embs_np, ids=item_data['item_id'].values)\n",
    "index.createIndex(print_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcula os k items mais similares baseado nas features dos nomes e calcula o recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = 0\n",
    "hs = list()\n",
    "for elist, t in tqdm.tqdm(train[['user_history', 'item_bought']].values):\n",
    "    #elist = json.loads(elist)\n",
    "    rep = list()\n",
    "    for e in elist:\n",
    "        # Se for view adiciona o embeding\n",
    "        if isinstance(e['event_info'], int) and e['event_info'] in item_data[\"item_id\"].values:\n",
    "            rep.append(item_emb_map[e['event_info']])\n",
    "            #print(item_title_map[e['event_info']])\n",
    "    # Calcula media por livra\n",
    "    h = np.mean(rep, axis=0)\n",
    "    #hs.append(h)\n",
    "    #h = rep[0]\n",
    "    #t = item_emb_map[t]\n",
    "    \n",
    "    #print()\n",
    "    try:\n",
    "        # Gera sugestoes\n",
    "        k = index.knnQuery(h, k=50)\n",
    "        # Adiciona ao recall se a compra esta nas sugestoes\n",
    "        recall += int(t in set(k[0]))\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    #for i,d in zip(k[0], k[1]):\n",
    "    #    print(d, item_title_map[i])\n",
    "    #print(recall)    \n",
    "    #print(int(t in k[0]))\n",
    "    #print()\n",
    "    \n",
    "    \n",
    "    #print(item_title_map[t])\n",
    "    #print(\"-\"*10+\"\\n\"*5)\n",
    "print(recall/train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall@10 - 0.13778097264275843\n",
    "recall@20 - 0.15457821731374785\n",
    "recall@100 - 0.18157240604797623\n",
    "recall@1000 - 0.18950632074992194\n",
    "recall cs = viewed - 0.29388401187908886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.saveIndex(\"22a_sbert_neuralmind.nms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, search_data):\n",
    "        self.search_data = search_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.search_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq_index = self.search_data[index][0]\n",
    "        search = self.search_data[index][1]\n",
    "        #print(search)\n",
    "        return seq_index, search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "search_data = set()\n",
    "seq_index = 0\n",
    "for hist, bought in tqdm.tqdm(train[['user_history', 'item_bought']].values):\n",
    "    \n",
    "    for item in hist:\n",
    "        i = item['event_info']\n",
    "        if item['event_type'] == 'search':\n",
    "            search_data.add((seq_index, i.lower()))   \n",
    "    seq_index += 1\n",
    "search_data = list(search_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "pretrained = 'neuralmind/bert-large-portuguese-cased'\n",
    "\n",
    "model = SentenceTransformer(pretrained,  device='cuda')\n",
    "train_data = Dataset(search_data)\n",
    "train_loader = DataLoader(train_data, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq_index_embs_map = np.zeros((train.shape[0], 1024))\n",
    "res = list()\n",
    "\n",
    "for seq_ix, search in tqdm.tqdm(train_loader):\n",
    "    #print(seq_i\n",
    "    #print(search_list)\n",
    "    emb = model.encode(search)\n",
    "    seq_ix = seq_ix.numpy()\n",
    "    for i in range(emb.shape[0]):\n",
    "        res.append((seq_ix[i], emb[i, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "ctr = Counter([e[0] for e in res])\n",
    "\n",
    "seq_index_embs_map = np.zeros((train.shape[0], 1024))\n",
    "for seqix, emb in tqdm.tqdm(res):\n",
    "    seq_index_embs_map[seqix, :] += emb\n",
    "\n",
    "for i in tqdm.tqdm(range(train.shape[0])):\n",
    "    seq_index_embs_map[i, :] /= ctr.get(i, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(seq_index_embs_map, \"22a_embs_search_np.pkl.z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "search_data = set()\n",
    "seq_index = 0\n",
    "for hist in tqdm.tqdm(test['user_history'].values):\n",
    "    \n",
    "    for item in json.loads(hist):\n",
    "        i = item['event_info']\n",
    "        if item['event_type'] == 'search':\n",
    "            search_data.add((seq_index, i.lower()))   \n",
    "    seq_index += 1\n",
    "search_data = list(search_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "pretrained = 'neuralmind/bert-large-portuguese-cased'\n",
    "\n",
    "model = SentenceTransformer(pretrained,  device='cuda')\n",
    "test_data = Dataset(search_data)\n",
    "test_loader = DataLoader(test_data, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq_index_embs_map = np.zeros((train.shape[0], 1024))\n",
    "res = list()\n",
    "\n",
    "for seq_ix, search in tqdm.tqdm(test_loader):\n",
    "    #print(seq_i\n",
    "    #print(search_list)\n",
    "    emb = model.encode(search)\n",
    "    seq_ix = seq_ix.numpy()\n",
    "    for i in range(emb.shape[0]):\n",
    "        res.append((seq_ix[i], emb[i, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "ctr = Counter([e[0] for e in res])\n",
    "\n",
    "seq_index_embs_map = np.zeros((test.shape[0], 1024))\n",
    "for seqix, emb in tqdm.tqdm(res):\n",
    "    seq_index_embs_map[seqix, :] += emb\n",
    "\n",
    "for i in tqdm.tqdm(range(test.shape[0])):\n",
    "    seq_index_embs_map[i, :] /= ctr.get(i, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(seq_index_embs_map, \"22a_embs_search_test_np.pkl.z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
