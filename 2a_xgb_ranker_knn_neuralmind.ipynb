{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import joblib as jb\n",
    "\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "import numba\n",
    "import dask\n",
    "import xgboost\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carrega os df's estruturados de view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"./data/22_train_view_melted.parquet\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(\"./data/22_test_view_melted.parquet\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gera três dicionarios em que a chave é o item_id e os valores são title, price e domain_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_data = pd.read_parquet(\"./data/item_data.parquet\")\n",
    "item_data.head()\n",
    "\n",
    "item_title_map = item_data[['item_id', 'title']].drop_duplicates()\n",
    "item_title_map = item_title_map.set_index(\"item_id\").squeeze().to_dict()\n",
    "\n",
    "item_price_map = item_data[['item_id', 'price']].drop_duplicates()\n",
    "item_price_map = item_price_map.set_index(\"item_id\").squeeze().to_dict()\n",
    "\n",
    "item_domain_map = item_data[['item_id', 'domain_id']].drop_duplicates()\n",
    "item_domain_map = item_domain_map.set_index(\"item_id\").squeeze().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importa df's de busca e adiciona features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_search = pd.read_parquet(\"./data/22_train_search_melted.parquet\")\n",
    "train_search.head()\n",
    "\n",
    "train_search['search_len'] = train_search['event_info'].str.len()\n",
    "train_search['num_words'] = train_search['event_info'].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "search_features = pd.DataFrame(index=train_search['seq_index'].unique())\n",
    "search_features['n_searches'] = train_search.groupby('seq_index').size()\n",
    "search_features['n_unique_searches'] = train_search.groupby('seq_index')['event_info'].nunique()\n",
    "search_features['avg_search_seqpos'] = train_search.groupby('seq_index')['seq_pos'].mean()\n",
    "search_features['avg_search_len'] = train_search.groupby('seq_index')['search_len'].mean()\n",
    "search_features['avg_search_words'] = train_search.groupby('seq_index')['num_words'].mean()\n",
    "search_features = search_features.reset_index().rename(columns={\"index\": \"seq_index\"})\n",
    "search_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_search = pd.read_parquet(\"./data/22_test_search_melted.parquet\")\n",
    "test_search.head()\n",
    "\n",
    "test_search['search_len'] = test_search['event_info'].str.len()\n",
    "test_search['num_words'] = test_search['event_info'].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "search_features_test = pd.DataFrame(index=test_search['seq_index'].unique())\n",
    "search_features_test['n_searches'] = test_search.groupby('seq_index').size()\n",
    "search_features_test['n_unique_searches'] = test_search.groupby('seq_index')['event_info'].nunique()\n",
    "search_features_test['avg_search_seqpos'] = test_search.groupby('seq_index')['seq_pos'].mean()\n",
    "search_features_test['avg_search_len'] = test_search.groupby('seq_index')['search_len'].mean()\n",
    "search_features_test['avg_search_words'] = test_search.groupby('seq_index')['num_words'].mean()\n",
    "search_features_test = search_features_test.reset_index().rename(columns={\"index\": \"seq_index\"})\n",
    "search_features_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['n_views'] = train.groupby(\"seq_index\")['viewed'].transform('sum')\n",
    "train['n_views_this'] = train.groupby([\"seq_index\", 'event_info'])['viewed'].transform(\"sum\")\n",
    "train['n_views_this_domain'] = train.groupby([\"seq_index\", 'item_domain'])['viewed'].transform(\"sum\")\n",
    "train['unique_items_viewed'] = train.groupby(\"seq_index\")['event_info'].transform('nunique')\n",
    "train['unique_domains_viewed'] = train.groupby(\"seq_index\")['item_domain'].transform(\"nunique\")\n",
    "train['n_views_this_ratio'] = train['n_views_this'] / train['n_views']\n",
    "train['n_views_this_ratio_domain'] = train['n_views_this'] / train['n_views_this_domain']\n",
    "\n",
    "train2 = pd.merge(train, search_features, on='seq_index', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['n_views'] = test.groupby(\"seq_index\")['viewed'].transform('sum')\n",
    "test['n_views_this'] = test.groupby([\"seq_index\", 'event_info'])['viewed'].transform(\"sum\")\n",
    "test['n_views_this_domain'] = test.groupby([\"seq_index\", 'item_domain'])['viewed'].transform(\"sum\")\n",
    "test['unique_items_viewed'] = test.groupby(\"seq_index\")['event_info'].transform(\"nunique\")\n",
    "test['unique_domains_viewed'] = test.groupby(\"seq_index\")['item_domain'].transform(\"nunique\")\n",
    "\n",
    "test['n_views_this_ratio'] = test['n_views_this'] / test['n_views']\n",
    "test['n_views_this_ratio_domain'] = test['n_views_this'] / test['n_views_this_domain']\n",
    "\n",
    "test2 = pd.merge(test, search_features_test, on='seq_index', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2.drop_duplicates(subset=['seq_index', \"event_info\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.drop_duplicates(subset=['seq_index', \"event_info\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gera a variável resposta (rank)\n",
    "\n",
    "1. Atribui 1 se o domain_id do item visto é o mesmo do item comprado, caso contrario, 0\n",
    "2. +12 se o item visto é o comprado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2['y_rank'] = (train2['bought_domain'] == train2['item_domain']).astype(int)\n",
    "train2['y_rank'] += (train2['bought_id'] == train2['event_info']).astype(int) * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2.to_parquet(\"./data/22c_train_melt_with_features.parquet\", engine='fastparquet', compression=None)\n",
    "test2.to_parquet(\"./data/22c_test_melt_with_features.parquet\", engine='fastparquet', compression=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stack gen\n",
    "\n",
    "- **Extrai o domain_id dos best sellers**\n",
    "- `pad`: se faltam k produtos pra completar o top 10, adiciona k a partir do best seller\n",
    "- `pad_str`: idem `pad` só que para o domain_id\n",
    "- `ndcg_vec`: calcula o ndcg@10 a partir de um vetor de predicao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "log_pos = np.log1p(np.arange(1,11))\n",
    "best_sellers = [1587422, 1803710,   10243,  548905, 1906937,  716822, 1361154, 1716388,  725371,  859574]\n",
    "best_sellers_domain = [item_domain_map[e] for e in best_sellers]\n",
    "\n",
    "def pad(lst):\n",
    "    \n",
    "    if len(lst) == 0:\n",
    "        return best_sellers\n",
    "    if len(lst) < 10:\n",
    "        lst += best_sellers[:(10 - len(lst))]\n",
    "    return np.array(lst)\n",
    "\n",
    "def pad_str(lst):\n",
    "    if len(lst) == 0:\n",
    "        return best_sellers_domain\n",
    "    if len(lst) < 10:\n",
    "        lst += best_sellers_domain[:(10 - len(lst))]\n",
    "    return lst\n",
    "\n",
    "# this is wrong, double counts exact item hits\n",
    "def ndcg_vec(ytrue, ypred, ytrue_domain, ypred_domain):\n",
    "    relevance = np.zeros((ypred.shape[0], 10))\n",
    "    for i in range(10):\n",
    "        relevance[:, i] = np.equal(ypred_domain[:, i], ytrue_domain) * (np.equal(ypred[:, i], ytrue) * 12 + 1)\n",
    "    dcg = (relevance / log_pos).sum(axis=1)\n",
    "\n",
    "    i_relevance = np.ones(10)\n",
    "    i_relevance[0] = 12.\n",
    "    idcg = np.zeros(ypred.shape[0]) + (i_relevance / log_pos).sum()\n",
    "\n",
    "    return (dcg / idcg).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.shape,tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_p.iloc[ts].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from cuml.preprocessing import TargetEncoder\n",
    "\n",
    "\n",
    "kf = GroupKFold(n_splits=2)\n",
    "stack_p = pd.DataFrame(index=train2.index, dtype=np.float64)\n",
    "for tr, ts in kf.split(train2, groups=train2['seq_index']):\n",
    "    Xtr = train2.iloc[tr]\n",
    "    Xval = train2.iloc[ts]\n",
    "    \n",
    "    \n",
    "    # ran once, for the first iteration of this loop, to reuse the \"same\" data in the stack\n",
    "    #joblib.dump(Xtr['seq_index'].unique(), \"./valid/fold1.pkl.z\")\n",
    "    #joblib.dump(Xval['seq_index'].unique(), \"./valid/fold2.pkl.z\")\n",
    "    \n",
    "    tgt_cuml = TargetEncoder(n_folds=5, smooth=5e-2)\n",
    "    for c in ['item_domain', 'event_info']:\n",
    "        Xtr[c+\"_cuml\"] = tgt_cuml.fit_transform(Xtr[c], Xtr['has_bought'])\n",
    "        Xval[c+\"_cuml\"] = tgt_cuml.transform(Xval[c])\n",
    "\n",
    "    features = ['item_price', 'seq_pos', 'n_views',\n",
    "           'n_views_this', 'n_views_this_domain', 'unique_items_viewed',\n",
    "           'unique_domains_viewed', 'item_domain_cuml', 'event_info_cuml', 'n_searches', 'n_unique_searches',\n",
    "           'avg_search_seqpos', 'avg_search_len', 'avg_search_words', 'n_views_this_ratio', 'n_views_this_ratio_domain', 'viewed']\n",
    "\n",
    "    params = [0.027652448846980884, 6, 1.5196450924014913, 0.15061222682840253, 0.4999203983793246]\n",
    "    learning_rate, max_depth, min_child_weight, subsample, colsample_bytree = params\n",
    "\n",
    "\n",
    "    Xtrr, ytr = Xtr[features], Xtr['y_rank']\n",
    "    Xvall = Xval[features]\n",
    "    \n",
    "    # Tamanho da query\n",
    "    groups = Xtr.groupby('seq_index').size().values\n",
    "\n",
    "    mdl = xgboost.XGBRanker(seed=0, tree_method='gpu_hist', gpu_id=0, n_estimators=1000,\n",
    "                               learning_rate=learning_rate, max_depth=max_depth, min_child_weight=min_child_weight,\n",
    "                                subsample=subsample, colsample_bytree=colsample_bytree, objective='rank:pairwise')\n",
    "\n",
    "    mdl.fit(Xtrr, ytr, group=groups)\n",
    "\n",
    "    p = mdl.predict(Xvall)\n",
    "    stack_p.iloc[ts] = p\n",
    "\n",
    "    preds = Xval[['seq_index', 'has_bought', 'item_domain', 'bought_domain', 'event_info', 'bought_id']].copy()\n",
    "    preds['p'] = p\n",
    "    preds = preds.sort_values('p', ascending=False).drop_duplicates(subset=['seq_index', 'event_info'])\n",
    "\n",
    "    ytrue = preds.groupby(\"seq_index\")['bought_id'].apply(lambda x: x.iloc[0]).values\n",
    "    ytrue_domain = preds.groupby(\"seq_index\")['bought_domain'].apply(lambda x: x.iloc[0]).values\n",
    "\n",
    "    ypred = preds.groupby(\"seq_index\")['event_info'].apply(lambda x: pad(x.iloc[:10].tolist()))\n",
    "    ypred = np.array(ypred.tolist())\n",
    "\n",
    "    ypred_domain = preds.groupby(\"seq_index\")['item_domain'].apply(lambda x: pad_str(x.iloc[:10].tolist()))\n",
    "    ypred_domain = np.array(ypred_domain.tolist())\n",
    "\n",
    "    print(ndcg_vec(ytrue, ypred, ytrue_domain, ypred_domain))\n",
    "pd.DataFrame(stack_p, columns=['22c']).to_parquet(\"./stack_2f/22c.parquet\", engine='fastparquet', compression=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.preprocessing import TargetEncoder\n",
    "\n",
    "#train3 = train2.drop_duplicates(subset=['seq_index', \"event_info\"])\n",
    "groups = train2.groupby('seq_index').size().values\n",
    "\n",
    "#test3 = test2.drop_duplicates(subset=['seq_index', \"event_info\"])\n",
    "\n",
    "tgt_cuml = TargetEncoder(n_folds=5, smooth=5e-2)\n",
    "for c in ['item_domain', 'event_info']:\n",
    "    train2[c+\"_cuml\"] = tgt_cuml.fit_transform(train2[c], train2['has_bought'])\n",
    "    test2[c+\"_cuml\"] = tgt_cuml.transform(test2[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['item_price', 'seq_pos', 'n_views',\n",
    "       'n_views_this', 'n_views_this_domain', 'unique_items_viewed',\n",
    "       'unique_domains_viewed', 'item_domain_cuml', 'event_info_cuml', 'n_searches', 'n_unique_searches',\n",
    "       'avg_search_seqpos', 'avg_search_len', 'avg_search_words', 'n_views_this_ratio', 'n_views_this_ratio_domain', 'viewed']\n",
    "params = [0.027652448846980884, 6, 1.5196450924014913, 0.15061222682840253, 0.4999203983793246]\n",
    "learning_rate, max_depth, min_child_weight, subsample, colsample_bytree = params\n",
    "mdl = xgboost.XGBRanker(seed=0, tree_method='gpu_hist', gpu_id=0, n_estimators=1000,\n",
    "                           learning_rate=learning_rate, max_depth=max_depth, min_child_weight=min_child_weight,\n",
    "                            subsample=subsample, colsample_bytree=colsample_bytree, objective='rank:pairwise')\n",
    "mdl.fit(train2[features], train2['y_rank'], group=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2[features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mdl.predict(test2[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(p, columns=['22c']).to_parquet(\"./stack_2f/22c_test.parquet\", engine='fastparquet', compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = test2[['seq_index', 'event_info']].copy()\n",
    "preds['p'] = p\n",
    "preds = preds.sort_values('p', ascending=False).drop_duplicates(subset=['seq_index', 'event_info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(lst):\n",
    "    pad_candidates = [1587422, 1803710,   10243,  548905, 1906937,  716822, 1361154, 1716388,  725371,  859574]\n",
    "    if len(lst) == 0:\n",
    "        return pad_candidates\n",
    "    if len(lst) < 10:\n",
    "        lst += [lst[0]] * (10 - len(lst)) # pad_candidates[:(10 - len(lst))]\n",
    "    return np.array(lst)\n",
    "ypred = preds.groupby(\"seq_index\")['event_info'].apply(lambda x: pad(x.iloc[:10].tolist()))\n",
    "seq_index = ypred.index\n",
    "ypred = np.array(ypred.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_final = np.zeros((177070, 10))\n",
    "ypred_final[seq_index, :] = ypred\n",
    "no_views = np.setdiff1d(np.arange(177070), seq_index)\n",
    "#ypred_final[no_views, :] = np.array([1587422, 1803710,   10243,  548905, 1906937,  716822, 1361154, 1716388,  725371,  859574])\n",
    "ypred_final = ypred_final.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#permite produtos repetidos\n",
    "pd.DataFrame(ypred_final).to_csv(\"./subs/22c.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['seq_index'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l ./subs/22c.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ./subs/22c.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
